---
title: "6.824 第一课：MapReduce"
date: 2019-02-27T16:42:50+08:00
lastmod: 2019-02-27T16:42:50+08:00
draft: true
keywords: []
tags: []
categories: []
author: "alei"
toc: true
comment: true
autoCollapseToc: false
---

# 分布式系统的导论

什么是分布式系统？多台**协作**的计算机构成的系统。

为什么要做分布式？lecture 中提了四点：

* to organize physically separate entities
* to achieve security via isolation
* to tolerate faults via replication
* to scale up throughput via parallel CPUs/mem/disk/net

1,2 的感受不是很深，就我而言，觉得核心还是在**容错**和**伸缩**这两点上。

比如第一课讲的 MapReduce，动辄 1T 的数据量是没法在合理的时间内由单机进行计算的（更何况 2004 年的时候 Google 的每个机器实例内存只有 2-4G）。为了 Scale up，必须得做分布式。

分布式系统的挑战：

* 复杂，大部分逻辑是并发的：Race Condition 普遍存在并且比单机时更难定位和解决；
* 必须解决 partial failure：分布式系统中错误是常态，节点宕机网络分区这种会导致部分节点不可用的问题是根植在分布式系统的本质中的；
* 难以实现物理资源的性能潜力：理想状况下，N 倍的物理资源应当带来 N倍的性能，但现实中这是不可能的，一方面系统规模越大，各种协调组件的 overhead 就越大；另一方面，随着规模上升，部分资源比如网络、共享存储会碰到瓶颈。而克服或缓解这些障碍，则是既有挑战，又回报丰厚的事情。

# MapReduce 论文小结

MapReduce 与 GFS、BigTable 并称为大数据的三驾马车，足见它在大数据领域的奠基地位。而如此重要的模型，却又异常的简单，实在让人赞叹。

首先，MapReduce 的要解决的问题是普遍存在的：数据量大到单机无法在可接受的时间里完成计算。在 MapReduce 之前，大家的办法是自己为计算逻辑实现分布式，每一个系统自己去搞分布式，成本是非常高的，这体现在两个方面：

* 一是人贵了，因为软件难维护，只有专家能写，只有专家能改；
* 二是开发量极速膨胀，任何 trivial 的逻辑都要搞一套分布式，分布式的逻辑比业务逻辑还多；

很明显，我们需要一个抽象，为用户封装掉底下的分布式、容错、性能优化细节，隐去"分布式计算"中的"分布式"三个字，让用户只关心"计算"。

而 MapReduce，就建立了一个模型来完成这层抽象。它本身简单到几行伪代码就可以描述：

```go
map(input) -> list(k, v)
reduce(k, list(v)) -> list(v)
```

其实这和函数式编程中的 map、reduce 模型是高度一致的。为了形象地讲清楚这个过程，我们现在用 MapReduce 解决一个问题：10T 的文件中，每行都是一条互联网用户的访问记录（时间、URL、地域...），现在我们希望算出每个网站的访问量。

你可能会想，那使用一个本地的 HashMap，以网站的域名为 Key，线性扫描一遍文件不就可以了？

假如真的是这样，那就不叫"单机无法在合理时间内解决"的问题了，上面这个方案有两个问题：

* 假设我们有 10G 个域名，这个 HashMap 单机存不下；
* 那不如换成 B-Tree，用磁盘来存储？且不说 10G 个 Entry 建 B-Tree 要何等巨量的存储，这个操作本身就太慢了，单机**无法在合理时间内完成**

现在用 MapReduce 来算。那么，任务的 Manager 会先将输入文件切分为 M 份，由 M 个 Task 来并行做 `map()` 操作，假设 M 是 200,000，每个任务的输入就只有 50M 了。

`map()` 中的具体逻辑是用户编写的，针对这个问题，我们的 `map()` 逻辑很简单:

```go
func map(content) {
    for line in content {
        // hostname() 函数用于提取出这一行的域名
        EmitIntermediate(hostname(line), 1)
    }
}
```

我们针对每行数据，提取出这一行访问记录中的"域名"部分，作为 key，而 value 1。`EmitIntermediate()` 就是提交 `map()` 结果（这些结果成为中间结果）的操作。MapReduce 框架收到结果后，会进行一个 `Shuffle` 操作：将 `map()` 提交的记录尽量均匀地分配到各个  R 个 `Reducer` 上，这里的分配策略是可插拔的，只满足两点即可：

* 分配尽可能均匀
* key 相同的记录需要落到到一个 `Reducer` 上

那最常见的就是用一个 hash 函数来分配了：hash(k2) / R。hash 后，框架会按 hash 结果把中间结果写到本地磁盘上的 R 个文件中(与 R 个 Reducer 一一对应），并将这些文件的信息提交给一个中央控制器。

接下来是 `reduce()` 操作，每个 `Reducer` 都会问中央控制器，分给自己的中间结果分别在哪些机器上叫什么文件，然后去对应的 `Map` 任务所在的机器上把它们读过来，读过来之后，需要按 key 进行一次**排序**，把相同的 key 收到一起，再扔给我们写的 `reduce()` 函数处理，`reduce()` 函数当然也很简单:

```go
// 这步传进来的 value 是一个迭代器，因为经过了 sort 之后，MapReduce 框架已经帮我们把相同 key 的值收集到一起迭代了
func reduce(hostname, valueIterator) {
    sum := 0
    for v in valueIterator {
        sum += v
    }
    Emit(v)
}
```
最后 `Emit()` 出来的自然就是最终结果：每个域名的访问次数

上面的过程中，还提到了一个"中央控制器"，这个组件在框架中称作 Manager，Reducer 到哪拿数据，Maper 是否存活的判断，都是 Manager 的逻辑。于是就有了论文中的图一。

# 统一的分布式计算模型

MapReduce 确实能解决问题，但它又为什么牛逼到可以成为分布式计算中非常基础的一种计算模型呢？

核心就在于这个模型不仅能算，还解决了分布式中最重要的容错和伸缩问题

首先是伸缩，MapReduce 理论上是无限水平伸缩的，Maper 和 Reducer 的数量 M，R 都可以无限水平扩展，并且带来线性增长的吞吐量（当然实际上会碰到带宽瓶颈，但现今同机房状况下，绝大多数场景都不会到达这个瓶颈）

其次是容错，Map 和 Reduce 都是纯函数，也就是没有副作用，它们接收一个不可变的输入，产生一个不可变的输出。因此无论单个 Map 和 Reduce 跑多少次，得到的结果都是一样的。因此，假如发生了部分节点宕机，那些任务重新计算即可。

# 优化：从模型到工业级框架

和 MapReduce 这个模型同样牛逼的还有它在实现上的众多工业级优化。

一是 Manager 在调度 Maper 节点时考虑了数据的 Locality。Google 在 MapReduce 上碰到的瓶颈是网络，因此 Manager 会考虑 GFS 上文件的存储位置，在切分文件之后尽量将 Maper 节点调度到与文件相同的机器上，避免通过网络去拉文件。

二是分布式系统中除了挂掉的机器之外，还有可能出现一些"掉队"的机器，掉队的原因千奇百怪，这样的机器能够正常响应心跳，看着是存活的，但就是比正常的机器慢很多倍。从某种程度上来讲，这种机器反而更有害。MapReduce 的策略是在只剩下少部分 worker 没有完成工作时（这些 worker 中很有可能存在掉队的机器），将这些 worker 复制一份到别的机器上跑，这样的话，一个子任务在两边都被分到掉队的机器上的概率就是非常小的。按 Google 的论文，这个小优化能够带来 40% 速度提升。

三是 Maper 侧的 Reducer 函数，Google 叫做 Combiner Function，在我们刚刚的例子里，Maper 要写非常多的元组到磁盘上，但对于 www.google.com 这样的域名，其实一次 `map()` 操作产生非常多条相关的记录，这时候假如我们在内存中预先 reduce 一样，把 100,000 条 (www.google.com, 1) reduce 成 1 条 (www.google.com, 100000) 再写到磁盘上，性能就大大提升了。

# 大数据框架的滥觞

MapReduce 之后，出现了一个开源实现 Hadoop（还有 GFS 的开源实现 HDFS，BigTable 的开源实现 HBase），大数据生态开始繁荣。但渐渐地，大家也发现 MapReduce 这套有不好使的地方：

* 性能问题，`map()` 和 `reduce()` 每次都是读文件，算完之后再写文件，假如一个复杂任务需要组合多个 `map()` 和 `reduce()`，那就是读读又写写。而大量中间数据其实都可以保存在内存中避免频繁而缓慢的文件系统操作。
* 对于机器学习问题，比如梯度下降，需要在数据集上做迭代，那上述的性能问题就更明显了
* 假如说数据不是一批给过来的，而是一块一块过来的，那也是对于每块要跑一遍 map() reduce()，太慢了

于是出现了 Spark 和 Storm，Flink。前者解决频繁读写文件的问题：引入一个抽象 RDD，直接根据算子推出计算的 DAG，将整个计算放到内存中，避免了中间结果的读写。而后两者则解决了"一块一块"过来的问题，也就是实时数据（流处理），怎么解决呢？Map Reduce 是让运算逻辑去找数据，找到了数据给整块来个处理，而流处理是让数据去找运算逻辑：将数据块中的每一行记录看做处理的原子对象，一个计算节点处理完就传给下一个节点。这些节点，Storm 中叫 Spout 和 Bolt，而 Flink 中叫 Operator。
