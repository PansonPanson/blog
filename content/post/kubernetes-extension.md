---
title: "在 Kubernetes 上构建应用：Kubernetes 扩展概览"
date: 2019-04-03T16:33:56+08:00
lastmod: 2019-04-03T16:33:56+08:00
draft: true
keywords: []
tags: []
categories: []
author: "alei"
toc: true
comment: true
autoCollapseToc: false
---

> 这篇文章总结自我在公司内的分享"Kubernetes Extensions Practice"，文章会尽力做到简洁易懂，不对读者老爷的背景知识做要求，~~假如你看不懂可以喷我~~。

在 Kubernetes(k8s) 的应用过程中，我们经常会碰到 k8s "不能用"（无法支持我们的需求）或者"不好用"（使用体验太差）的情况。作为一个通用的开源系统，k8s 不可能去适配各种迥然不同甚至彼此间会互相矛盾的定制化需求。但这些需求又是客观存在不可避免的，假如不予支持，社区就会被迫 fork 一份 k8s 的代码做定制化开发，造成整个生态中 API 标准的碎片化。这显然是 k8s 社区不想看到的，为此，k8s 设计了丰富的扩展点和强大的扩展模式，通常来说，当我们碰到 k8s 无法支持的场景时，k8s 扩展(极大概率)能够帮助我们解决问题。而这些扩展，也正是我们在 k8s 上构建复杂应用（相对于简单的无状态应用）的基石。

# Kubernetes 回顾

K8s 的最初定位是"容器编排平台"，直白地说，就是在一批机器上运行并管理我们的应用，而我们的应用则需要以"容器"这种标准形式进行交付，以便于 k8s 做这种统一的管理。下面是 k8s 的架构图（来自官方文档）：

那具体怎么管理呢？k8s 把它分解成了两个问题：

一是让用户描述自己想要一个怎样的集群，比如：

* 集群里有哪几个应用
* 各自应该有几个副本
* 这些副本使用多少CPU、多少内存

这些描述，我们称为集群的**期望状态**，k8s 以 Restful API 的方式来管理期望状态，比如我可以提交这样一个对象(yaml)：

这个对象声明：我们需要跑一个 ReplicaSet(副本集)，这个副本集使用的容器是 `nginx`，并且拥有三个副本。可见，k8s 的 API 并不表达"去做什么"，而是表达"我想要什么"，这种范式就称作"声明式 API"

有了期望状态之后，第二个问题就是如何达成用户的期望状态。这里就涉及第二个范式"**控制循环(ControlLoop)**"（或者说同步循环, SyncLoop）。所谓控制循环，其实就是我们不断检查集群的**期望状态**与**现实状态**是否一致，假如不一致，就触发一系列操作来实现现实状态向期望状态的转化，这些操作在 k8s 中成为"调谐"(reconcile)。我们可以用一段伪代码来描述这个过程:

以上面的 ReplicaSet 为例，控制循环中发现副本数不够，这个不够可能是副本因为 RuntimeError 挂掉了，也可能是因为集群中挂了一个节点，殃及了一个副本。无论如何，总之我们没有达成期望状态，于是在调谐操作中，我们就向 k8s 提交一个新副本，让副本数达到期望。

你可能会想，那为什么我们提交一个副本就会真的有一个副本运行起来呢？这当然也是**控制循环**，就和负责 ReplicaSet 的循环一个道理，k8s 中也存在针对应用实例，也就是 Pod 对象的控制循环：调度器发现有一个新 Pod 出现，但没有绑定到任何节点上，不符合期望，那就跑一边调度算法，给它填上一个节点；节点上的 Agent 进程，也就是 kubelet，发现有一个 Pod 属于自己，但是节点上并没有真的运行这个 Pod，不符合期望状态，于是就把它在节点上拉起来。

除此之外，还有非常多的例子，比如 kube-proxy 这个组件就专门 watch service 这种对象，保证节点上的 iptables 规则和 service 对象声明的期望状态一致。这些控制循环有些是更新 k8s 本身的状态（比如提交一个 Pod），有些是更新外部资源的状态（比如拉起一个容器或者修改 iptables），这些逻辑在本质上没有区别。而在命名上，我们则将读取 k8s API 对象再更新 k8s API 对象的组件叫做**控制器**，Controller。

说到这里，k8s 的两大核心"声明式 API"与"控制循环"就都清楚了，剩下的事情就是定义更多的 API 对象（承载更多形式的期望状态）并实现对应的控制循环了。比如说，我们的期望状态是这个应用每隔1小时跑一次，跑完就退出。但是没有任何 API 对象能够描述这个状态，于是我们就可以设计一个 CronJob 这样的对象来描述。

# Kubernetes 扩展模式

有了理论基础，我们就可以深入到扩展当中了。Kubernetes 的扩展总体分为两种模式：Hook 和 Controller：

Hook 是 k8s 的各个组件中通过代码预先写好的扩展点，k8s 会在走到某段逻辑时，调用我们配置的 Hook。由此，我们就将自定义逻辑集成到了 k8s 中。

根据调用方式，Hook 又分为两种常见模式，Binary Plugin 和 Webhook，顾名思义，Binary Plugin 是以可执行文件的方式被 k8s 组件直接通过启动子进程的方式进行调用的，而 Webhook 则通过网络接受 k8s 组件的调用。我们可以看到，Hook 模式的特点是被动接受调用，并且一定要 k8s 本身提供了这个扩展点才行。

通常来说，Hook 都有某种发现机制，大概分为：

* 代码写死发现规则：
* 通过命令行参数或配置文件进行配置：
* 基于 API 对象动态发现：

Controller 模式我们在上一节已经提过了，所谓 Controller，其实就是一个 k8s 的 client。它 watch API 对象来获得期望状态，并通过各种方式收集实际状态，最后通过自定义的 reconcile 逻辑实现实际状态到期望状态的趋近，而 reconcile 中做的操作往往也就是创建或更新几个 API 对象。因此直白地说，controller 就是一个一直运行着的 k8s client 进程。

仅仅是 watch k8s 的原生对象带来的扩展性并不大，因为这些对象本身的字段我们无法控制，也就无法描述我们自定义的状态。但是 k8s 还允许我们自定义自己的资源对象，因此，就像 k8s 用 CronJob、DaemonSet 对象来实现各种特殊编排逻辑一样，我们也可以定义自己的资源对象，来描述自定义的期望状态。这时，自定义 Controller 的作用就凸显出来了：**自定义的资源对象不可能自己到达期望状态，我们需要在自定义 Controller 中用控制循环来实现这点**。在实践中，这个模式强大且通用，k8s 社区中的大量项目都是基于这个模式构建的，我们也会在下一节进行更详细地说明。

# Kubernetes 扩展点与实际应用

以 Hook 和 Controller 为线索，我们先看一张 k8s 扩展点的全景视图，并逐个说明：

1. Kubectl Plugin: Kubectl 插件，kubectl 硬编码了一个插件发现机制，以 "kubectl-{command}" 形式名字命名的可执行文件就会被认为是 kubectl 插件，输入 "kubectl command" 就可以进行调用（其实这个机制有点幽默...）
2. Authentication(AuthN) Webhook: AuthN 即认证，也就是检查请求者能否"证明自己是某人"。当我们需要集成第三方的身份认证服务时， AuthN Webhook 就很有用了。但是，由于 webhook 是同步调用，会影响 latency，因此应用更广的是 Open ID Connect 这种扩展方式。这种扩展方式会从第三方身份认证服务那边获取公钥，用来验证请求中包含的 JWT 是否确实由第三方服务签发。
    * AuthN Webhook 例子：[AppCodes - Guard]
    * Open ID Connect 例子：[CoreOS - Dex]
3. Authorization(AuthZ) Webhook: AuthZ 即鉴权，也就是检查"你有没有权限做某事"(AuthN 环节已经证明你是谁了)。AuthZ 的扩展其实很好，因为 k8s 本身的 RBAC（Role-based access control，根据你所属的角色来判断你有没有权限做某事）机制已经很完善了，甚至于 Istio 项目完全复用了这套 RBAC 机制来做服务间的鉴权。
    * AuthZ 例子：[Yahoo - Garm]
4. Aggregation Apiserver: k8s 的 ApiServer 已经实现了微服务化，

# 结语
